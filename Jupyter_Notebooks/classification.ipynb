{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLynEWx6NEiK"
      },
      "source": [
        "# Imports and Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_Uo-KT0cVgQ",
        "outputId": "362c3516-275e-44c0-df55-83249e2d9cfd"
      },
      "outputs": [],
      "source": [
        "#To connect google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0k-9ri7JaAX"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import string\n",
        "\n",
        "import html\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "import os.path\n",
        "import pandas as pd\n",
        "from math import inf\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import statistics as st\n",
        "from tabulate import tabulate\n",
        "from IPython.display import clear_output\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import KFold,train_test_split\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "#NN\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "seed=22\n",
        "def reset_seeds(seed=seed):\n",
        "  keras.utils.set_random_seed(seed)\n",
        "  tf.random.set_seed(seed)\n",
        "  tf.keras.utils.set_random_seed(seed)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  tf.config.experimental.enable_op_determinism()\n",
        "  os.environ['PYTHONHASHSEED']=str(seed)\n",
        "reset_seeds()\n",
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "!pip install datasets\n",
        "import datasets\n",
        "\n",
        "# !pip install sentence-transformers\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "\n",
        "clear_output()\n",
        "# !pip install nltk\n",
        "# !pip install pandas\n",
        "# !pip install tabulate\n",
        "# !pip install matplotlib\n",
        "# !pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaEX8IYBJZSN"
      },
      "outputs": [],
      "source": [
        "# Read input data\n",
        "# Input Data\n",
        "\n",
        "# Original\n",
        "# rt_reviews_data = pd.read_csv('./rotten_tomatoes_movie_reviews.csv')\n",
        "\n",
        "#Processed\n",
        "# rt_reviews_data = pd.read_csv('./rt_reviews_data_processed.csv')\n",
        "\n",
        "#Sampled\n",
        "# rt_reviews_data = pd.read_csv('./rt_reviews_data_sampled.csv')\n",
        "\n",
        "# rt_movies_data = pd.read_csv('./rotten_tomatoes_movies.csv')\n",
        "\n",
        "#IMDB\n",
        "rt_reviews_data = pd.read_csv('./IMDB Dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MFmwJT-1adg",
        "outputId": "72edc74f-9603-4dfc-eda2-0a6ff935a6e5"
      },
      "outputs": [],
      "source": [
        "# rt_reviews_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuVH0bQyBQXj"
      },
      "outputs": [],
      "source": [
        "keykey='rt'\n",
        "if('review' in rt_reviews_data.keys()):\n",
        "    keykey='imdb'\n",
        "    rt_reviews_data['reviewText']=rt_reviews_data['review']\n",
        "    rt_reviews_data['scoreSentiment']=rt_reviews_data['sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26QkGoomrxhO"
      },
      "outputs": [],
      "source": [
        "# Subset of Original data\n",
        "rt_input_data = rt_reviews_data[['reviewText','scoreSentiment']]\n",
        "\n",
        "# Remove Null columns\n",
        "rt_input_data = rt_input_data.dropna(subset=['reviewText', 'scoreSentiment'])\n",
        "\n",
        "# Convert html symbols into text\n",
        "rt_input_data['reviewText'] = rt_input_data['reviewText'].apply(lambda x: html.unescape(x))\n",
        "\n",
        "# Save processed data for future reuse.\n",
        "# rt_input_data.to_csv('/content/drive/MyDrive/DO NOT DELETE/SHARE/rt_reviews_data_processed.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6Q5rFNBYYNf"
      },
      "source": [
        "# Utility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCNwxieEYopR"
      },
      "source": [
        "### Glove model utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc2zThmWjSoo"
      },
      "outputs": [],
      "source": [
        "# Create custom Vectorizer using glove model\n",
        "\n",
        "class Word2VecVectorizer:\n",
        "  def __init__(self, model):\n",
        "    self.word_vectors = model\n",
        "  def fit(self, data):\n",
        "    pass\n",
        "\n",
        "  def transform(self, data):\n",
        "    v = self.word_vectors.get_vector('king')\n",
        "    self.D = v.shape[0]\n",
        "\n",
        "    X = np.zeros((len(data), self.D))\n",
        "    n = 0\n",
        "    emptycount = 0\n",
        "    for sentence in data:\n",
        "      tokens = sentence.split()\n",
        "      vecs = []\n",
        "      m = 0\n",
        "      for word in tokens:\n",
        "        try:\n",
        "          vec = self.word_vectors.get_vector(word)\n",
        "          vecs.append(vec)\n",
        "          m += 1\n",
        "        except KeyError:\n",
        "          pass\n",
        "      if len(vecs) > 0:\n",
        "        vecs = np.array(vecs)\n",
        "        X[n] = vecs.mean(axis=0)\n",
        "      else:\n",
        "        emptycount += 1\n",
        "      n += 1\n",
        "    return X\n",
        "\n",
        "  def fit_transform(self, data):\n",
        "    self.fit(data)\n",
        "    return self.transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrEw4_Rgl_Zd"
      },
      "outputs": [],
      "source": [
        "# Encode training and test data using glove model\n",
        "glove_encodings_fetched=None\n",
        "\n",
        "if(os.path.exists('/content/drive/MyDrive/DO NOT DELETE/SHARE/glove_encodings_project_{}.txt'.format(keykey))):\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/glove_encodings_project_{}.txt'.format(keykey),'r') as json_file:\n",
        "        glove_encodings_fetched = json.load(json_file)\n",
        "else:\n",
        "    # load the Stanford GloVe model\n",
        "    glove_path='/content/drive/MyDrive/glove.6B.100d.txt' #Colab\n",
        "    # glove_path='./glove.6B.100d.txt' #Local\n",
        "    glove_model_data = KeyedVectors.load_word2vec_format(glove_path, binary=False, no_header=True)\n",
        "    def X_tensor_glove_original(corpus):\n",
        "        corpus=corpus.copy()\n",
        "        vectorizer = Word2VecVectorizer(glove_model_data)\n",
        "        X_train = vectorizer.fit_transform(corpus)\n",
        "        X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "        return X_train\n",
        "\n",
        "    input_converted_glove=X_tensor_glove_original(rt_reviews_data['reviewText'])\n",
        "\n",
        "    glove_encodings_fetched={}\n",
        "\n",
        "    for i in range(rt_reviews_data.shape[0]):\n",
        "        glove_encodings_fetched[rt_reviews_data['reviewText'][i]]=input_converted_glove[i].numpy().tolist()\n",
        "\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/glove_encodings_project_{}.txt'.format(keykey),'w') as fp:\n",
        "        fp.write(json.dumps(glove_encodings_fetched))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezEhrPeIlbRq"
      },
      "source": [
        "### bert model utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7lrafUOSIfP"
      },
      "outputs": [],
      "source": [
        "# Encode training and test data using all-distilroberta-v1 bert model\n",
        "bert_encodings_fetched=None\n",
        "\n",
        "if(os.path.exists('/content/drive/MyDrive/DO NOT DELETE/SHARE/bert_encodings_project_{}.txt'.format(keykey))):\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/bert_encodings_project_{}.txt'.format(keykey),'r') as json_file:\n",
        "        bert_encodings_fetched = json.load(json_file)\n",
        "else:\n",
        "    # Load all-distilroberta-v1 model\n",
        "    bert_model = SentenceTransformer('all-distilroberta-v1')\n",
        "    clear_output()\n",
        "    def X_tensor_bert_original(corpus):\n",
        "        corpus=corpus.copy()\n",
        "        X_train = bert_model.encode(corpus)\n",
        "        X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "        return X_train\n",
        "\n",
        "    input_converted_bert=X_tensor_bert_original(rt_reviews_data['reviewText'])\n",
        "\n",
        "    bert_encodings_fetched={}\n",
        "\n",
        "    for i in range(rt_reviews_data.shape[0]):\n",
        "        bert_encodings_fetched[rt_reviews_data['reviewText'][i]]=input_converted_bert[i].numpy().tolist()\n",
        "\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/bert_encodings_project_{}.txt'.format(keykey),'w') as fp:\n",
        "        fp.write(json.dumps(bert_encodings_fetched))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jCKm7eQyUrY"
      },
      "source": [
        "### mpnet model utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9icKvX5yTrx"
      },
      "outputs": [],
      "source": [
        "# Encode training and test data using all-mpnet-base-v2 mpnet model\n",
        "mpnet_encodings_fetched=None\n",
        "\n",
        "if(os.path.exists('/content/drive/MyDrive/DO NOT DELETE/SHARE/mpnet_encodings_project_{}.txt'.format(keykey))):\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/mpnet_encodings_project_{}.txt'.format(keykey),'r') as json_file:\n",
        "        mpnet_encodings_fetched = json.load(json_file)\n",
        "else:\n",
        "    # Load all-mpnet-base-v2 model\n",
        "    mpnet_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "    clear_output()\n",
        "    def X_tensor_mpnet_original(corpus):\n",
        "        corpus=corpus.copy()\n",
        "        X_train = mpnet_model.encode(corpus)\n",
        "        X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "        return X_train\n",
        "\n",
        "    input_converted_mpnet=X_tensor_mpnet_original(rt_reviews_data['reviewText'])\n",
        "\n",
        "    mpnet_encodings_fetched={}\n",
        "\n",
        "    for i in range(rt_reviews_data.shape[0]):\n",
        "        mpnet_encodings_fetched[rt_reviews_data['reviewText'][i]]=input_converted_mpnet[i].numpy().tolist()\n",
        "\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/mpnet_encodings_project_{}.txt'.format(keykey),'w') as fp:\n",
        "        fp.write(json.dumps(mpnet_encodings_fetched))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADnnvdZ2KMc3"
      },
      "source": [
        "### Count Vectorizer utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "art1xsWXG-H3"
      },
      "outputs": [],
      "source": [
        "# Encode training and test data using countvec model\n",
        "countvec_encodings_fetched=None\n",
        "\n",
        "if(os.path.exists('/content/drive/MyDrive/DO NOT DELETE/SHARE/countvec_encodings_project_{}.txt'.format(keykey))):\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/countvec_encodings_project_{}.txt'.format(keykey),'r') as json_file:\n",
        "        countvec_encodings_fetched = json.load(json_file)\n",
        "else:\n",
        "    class LemmaTokenizer:\n",
        "      def __init__(self):\n",
        "          self.wnl = WordNetLemmatizer()\n",
        "      def __call__(self, doc):\n",
        "          return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
        "\n",
        "    def X_tensor_countvec_original(corpus):\n",
        "      corpus=corpus.copy()\n",
        "      vectorizer = CountVectorizer(max_features=1000, stop_words='english', lowercase=True, tokenizer=LemmaTokenizer())\n",
        "      X_train = vectorizer.fit_transform(corpus)\n",
        "      X_train=X_train.toarray()\n",
        "      X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "      return X_train\n",
        "\n",
        "    input_converted_countvec=X_tensor_countvec_original(rt_reviews_data['reviewText'])\n",
        "\n",
        "    countvec_encodings_fetched={}\n",
        "\n",
        "    for i in range(rt_reviews_data.shape[0]):\n",
        "        countvec_encodings_fetched[rt_reviews_data['reviewText'][i]]=input_converted_countvec[i].numpy().tolist()\n",
        "\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/countvec_encodings_project_{}.txt'.format(keykey),'w') as fp:\n",
        "        fp.write(json.dumps(countvec_encodings_fetched))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mjnOwK8KNsv"
      },
      "source": [
        "### TFIDF utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzdGtQUmJW4l"
      },
      "outputs": [],
      "source": [
        "# Encode training and test data using tfidf model\n",
        "tfidf_encodings_fetched=None\n",
        "\n",
        "if(os.path.exists('/content/drive/MyDrive/DO NOT DELETE/SHARE/tfidf_encodings_project_{}.txt'.format(keykey))):\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/tfidf_encodings_project_{}.txt'.format(keykey),'r') as json_file:\n",
        "        tfidf_encodings_fetched = json.load(json_file)\n",
        "else:\n",
        "    class LemmaTokenizer:\n",
        "      def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "      def __call__(self, doc):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
        "\n",
        "    def X_tensor_tfidf_original(corpus):\n",
        "      corpus=corpus.copy()\n",
        "      vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', lowercase=True, tokenizer=LemmaTokenizer())\n",
        "      X_train = vectorizer.fit_transform(corpus)\n",
        "      X_train=X_train.toarray()\n",
        "      X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "      return X_train\n",
        "\n",
        "    input_converted_tfidf=X_tensor_tfidf_original(rt_reviews_data['reviewText'])\n",
        "\n",
        "    tfidf_encodings_fetched={}\n",
        "\n",
        "    for i in range(rt_reviews_data.shape[0]):\n",
        "        tfidf_encodings_fetched[rt_reviews_data['reviewText'][i]]=input_converted_tfidf[i].numpy().tolist()\n",
        "\n",
        "    with open('/content/drive/MyDrive/DO NOT DELETE/SHARE/tfidf_encodings_project_{}.txt'.format(keykey),'w') as fp:\n",
        "        fp.write(json.dumps(tfidf_encodings_fetched))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP44VjJ4lhfA"
      },
      "source": [
        "### General Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoaWzqJCgNAf",
        "outputId": "cf6cd508-6163-4a3f-f716-5ac2de07b7ed"
      },
      "outputs": [],
      "source": [
        "# Create Dictionary to convert category into 1-hot vector and vice verse\n",
        "category_to_vector={}\n",
        "vector_to_category={}\n",
        "\n",
        "index=0\n",
        "set_of_list_categories=set(list(rt_reviews_data['scoreSentiment']))\n",
        "set(list(rt_reviews_data['scoreSentiment']))\n",
        "for i in list(set_of_list_categories):\n",
        "  newarray=[0]*len(set_of_list_categories)\n",
        "  newarray[index]=1\n",
        "  category_to_vector[i]=newarray\n",
        "  vector_to_category[index]=i\n",
        "  index+=1\n",
        "\n",
        "# print(category_to_vector)\n",
        "# print(vector_to_category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2tgRkBEW9E5"
      },
      "outputs": [],
      "source": [
        "# utility functions to convert category into 1-hot vector and vice verse\n",
        "def Y_tensor(Y_train):\n",
        "  Y_train=Y_train.copy()\n",
        "\n",
        "  for i in range(len(Y_train)):\n",
        "    Y_train[i]=category_to_vector[Y_train[i]]\n",
        "\n",
        "  Y_train=np.array(Y_train)\n",
        "  Y_train=tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
        "  return Y_train\n",
        "\n",
        "def vec2cat(input):\n",
        "    categories = []\n",
        "    for i in range(len(input)):\n",
        "        categories.append(vector_to_category[np.argmax(input[i])])\n",
        "    return categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awEeTeYtbRhI"
      },
      "outputs": [],
      "source": [
        "# Neural Network Model\n",
        "def generateNNModel(learning_rate, input_dim, optimizer_name):\n",
        "  reset_seeds()\n",
        "  model = Sequential()\n",
        "  model.add(keras.Input(shape=(input_dim,)))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(len(set_of_list_categories), activation='softmax'))\n",
        "  optimzer=None\n",
        "  if optimizer_name=='adam':\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "  elif optimizer_name=='sgd':\n",
        "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "  elif optimizer_name=='rmsprop':\n",
        "    optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ix4hv0zgj3BX"
      },
      "outputs": [],
      "source": [
        "# initialize array to save the various features statistics\n",
        "all_train_acc=[]\n",
        "all_train_std=[]\n",
        "all_train_f1=[]\n",
        "all_train_auc=[]\n",
        "\n",
        "all_val_acc=[]\n",
        "all_val_std=[]\n",
        "all_val_f1=[]\n",
        "all_val_auc=[]\n",
        "def reset_stat_array():\n",
        "  global all_train_acc,all_train_std,all_val_acc,all_val_std, all_train_f1, all_val_f1, all_train_auc, all_val_auc\n",
        "  all_train_acc=[]\n",
        "  all_train_std=[]\n",
        "  all_train_f1=[]\n",
        "  all_train_auc=[]\n",
        "\n",
        "  all_val_acc=[]\n",
        "  all_val_std=[]\n",
        "  all_val_f1=[]\n",
        "  all_val_auc=[]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHwafw5SbczO"
      },
      "outputs": [],
      "source": [
        "#A common function to run Neural Network on training data and test data. Uses arguments to change various parameters.\n",
        "def run_nn(X,Y, input_size, nn_epochs, filename_suffix, run_test_data, learning_rate, optimizer_name):\n",
        "    #Kfold\n",
        "    def kfold_analysis(train_val_X,train_val_y):\n",
        "        kf = KFold(n_splits = 5)\n",
        "        train_acc = []\n",
        "        val_acc = []\n",
        "\n",
        "        train_f1 = []\n",
        "        val_f1 = []\n",
        "\n",
        "        train_auc = []\n",
        "        val_auc = []\n",
        "        for train_index, val_index in tqdm(kf.split(train_val_X)):\n",
        "            train_X = train_val_X[train_index[0]:train_index[-1]+1]\n",
        "            train_y = train_val_y[train_index[0]:train_index[-1]+1]\n",
        "\n",
        "            val_X = train_val_X[val_index[0]:val_index[-1]+1]\n",
        "            val_y = train_val_y[val_index[0]:val_index[-1]+1]\n",
        "\n",
        "            # train_X_tensor=X_tensor(train_X)\n",
        "            #train\n",
        "            nn_model = generateNNModel(learning_rate, input_size,optimizer_name)\n",
        "            reset_seeds()\n",
        "            train_X=tf.convert_to_tensor(train_X, dtype=tf.float32)\n",
        "            history = nn_model.fit(train_X, Y_tensor(train_y), epochs=nn_epochs, batch_size=64,verbose=0)\n",
        "\n",
        "            #accuracy of training\n",
        "            reset_seeds()\n",
        "            training_data_predicted_values=nn_model.predict(train_X,verbose=0)\n",
        "            # print(training_data_predicted_values)\n",
        "            training_data_predicted_categories = vec2cat(training_data_predicted_values)\n",
        "\n",
        "            # print(set(train_y), set(training_data_predicted_categories))\n",
        "            train_acc.append(accuracy_score(train_y, training_data_predicted_categories))\n",
        "            train_f1.append(f1_score(train_y, training_data_predicted_categories, pos_label='positive'))\n",
        "            train_auc.append(roc_auc_score(train_y, training_data_predicted_values[:, 0]))\n",
        "\n",
        "            #accuracy of validation\n",
        "            reset_seeds()\n",
        "            val_X=tf.convert_to_tensor(val_X, dtype=tf.float32)\n",
        "            validation_data_predicted_values=nn_model.predict(val_X,verbose=0)\n",
        "            validation_data_predicted_categories = vec2cat(validation_data_predicted_values)\n",
        "\n",
        "            # print(st(val_y), set(validation_data_predicted_categories))\n",
        "            val_acc.append(accuracy_score(val_y, validation_data_predicted_categories))\n",
        "            val_f1.append(f1_score(val_y, validation_data_predicted_categories, pos_label='positive'))\n",
        "            val_auc.append(roc_auc_score(val_y, validation_data_predicted_values[:, 0]))\n",
        "\n",
        "        avg_train_acc = sum(train_acc) / len(train_acc)\n",
        "        avg_val_acc = sum(val_acc) / len(val_acc)\n",
        "\n",
        "        avg_train_f1 = sum(train_f1) / len(train_f1)\n",
        "        avg_val_f1 = sum(val_f1) / len(val_f1)\n",
        "\n",
        "        avg_train_auc = sum(train_auc) / len(train_auc)\n",
        "        avg_val_auc = sum(val_auc) / len(val_auc)\n",
        "\n",
        "        print(\"\\nThe average training accuracy is {} with standard deviation of {}\".format(avg_train_acc,st.pstdev(train_acc)))\n",
        "        print(\"The average validation accuracy is {} with standard deviation of {}\".format(avg_val_acc,st.pstdev(val_acc)))\n",
        "\n",
        "        return avg_train_acc, st.pstdev(train_acc), avg_val_acc, st.pstdev(val_acc), avg_train_f1, avg_val_f1, avg_train_auc, avg_val_auc\n",
        "    return kfold_analysis(X,Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpsmP1gZYb5T"
      },
      "source": [
        "# Feature Extraction and NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP-JBJOfJwuU"
      },
      "source": [
        "### Utility\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzIndmeXJv4G"
      },
      "outputs": [],
      "source": [
        "def X_tensor_countvec(corpus):\n",
        "  X_train=[]\n",
        "  for i in corpus:\n",
        "    X_train.append(countvec_encodings_fetched[i])\n",
        "  X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "  return X_train\n",
        "\n",
        "X_countvec=X_tensor_countvec(rt_reviews_data['reviewText'])\n",
        "Y=rt_reviews_data['scoreSentiment']\n",
        "\n",
        "\n",
        "\n",
        "def X_tensor_tfidf(corpus):\n",
        "  X_train=[]\n",
        "  for i in corpus:\n",
        "    X_train.append(tfidf_encodings_fetched[i])\n",
        "  X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "  return X_train\n",
        "\n",
        "\n",
        "X_tfidf=X_tensor_tfidf(rt_reviews_data['reviewText'])\n",
        "\n",
        "def X_tensor_glove(corpus):\n",
        "  X_train=[]\n",
        "  for i in corpus:\n",
        "    X_train.append(glove_encodings_fetched[i])\n",
        "  X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "  return X_train\n",
        "\n",
        "X_glove=X_tensor_glove(rt_reviews_data['reviewText'])\n",
        "\n",
        "\n",
        "def X_tensor_bert(corpus):\n",
        "  corpus=corpus.copy()\n",
        "  X_train=[]\n",
        "  for i in corpus:\n",
        "    X_train.append(bert_encodings_fetched[i])\n",
        "  X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "  return X_train\n",
        "\n",
        "X_bert=X_tensor_bert(rt_reviews_data['reviewText'])\n",
        "\n",
        "\n",
        "\n",
        "def X_tensor_mpnet(corpus):\n",
        "  corpus=corpus.copy()\n",
        "  X_train=[]\n",
        "  for i in corpus:\n",
        "    X_train.append(mpnet_encodings_fetched[i])\n",
        "  X_train=tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "  return X_train\n",
        "\n",
        "\n",
        "X_mpnet=X_tensor_mpnet(rt_reviews_data['reviewText'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw2IDx9Xyc7T"
      },
      "source": [
        "### CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXHAXtY0kKlf",
        "outputId": "757568fb-9b08-4046-f9d7-4ab6d392a4bb"
      },
      "outputs": [],
      "source": [
        "# Run Neural Network using Count Vectorizer feature\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = run_nn(list(X_countvec),list(Y), 1000, 10, 'countvec', False, 0.001, 'adam')\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr0BpG5iyamL"
      },
      "source": [
        "### TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdIjeZWmsXn3",
        "outputId": "90d255ff-7738-40dd-ebc5-d29d1008782c"
      },
      "outputs": [],
      "source": [
        "# Run Neural Network using TFIDF Vectorizer feature\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = run_nn(list(X_tfidf),list(Y), 1000, 15, '_tfidf', False, 0.001, 'adam')\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi1prRnGyYNr"
      },
      "source": [
        "### Glove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU9alRnltNnD",
        "outputId": "54ee27a1-a21a-4a01-e834-562d08924edc"
      },
      "outputs": [],
      "source": [
        "# Run Neural Network using Glove Vectorizer feature\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = run_nn(list(X_glove),list(Y), 100, 50, '_glove', False, 0.001, 'adam')\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7jOY_RNlIw5"
      },
      "source": [
        "### Bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1vZfudulA7G",
        "outputId": "ae6a541f-313c-4501-fa70-3d9fae94cfe6"
      },
      "outputs": [],
      "source": [
        "# Run Neural Network using BERT Vectorizer feature\n",
        "dimension_sbert=768\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = run_nn(list(X_bert),list(Y), dimension_sbert, 15, '_bert', False, 0.001, 'adam')\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITc-IDpHdjwC"
      },
      "source": [
        "### MPNET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdz4_Ulg1clG",
        "outputId": "aac3d6a5-8dbf-4bc9-890a-35486f15a636"
      },
      "outputs": [],
      "source": [
        "# Run Neural Network using MPnet Vectorizer feature\n",
        "dimension_mpnet=768\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = run_nn(list(X_mpnet),list(Y), dimension_mpnet, 15, '_mpnet', False, 0.001, 'adam')\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcZVpGGpXFJb",
        "outputId": "71b73d65-bcc3-4f1f-c11e-6c09e9041990"
      },
      "outputs": [],
      "source": [
        "# Create a table to compare the statistics\n",
        "def drawTable(all_val_acc,all_val_f1,all_val_auc):\n",
        "    table={\"Method\":['Count Vectorizer','TFIDF','GloVe','BERT','MPNET'],\"Accuracy\":all_val_acc,\"F1 Score\":all_val_f1,\"AUC\":all_val_auc}\n",
        "    print(tabulate(table , headers=\"keys\", tablefmt=\"grid\"))\n",
        "\n",
        "drawTable(all_val_acc,all_val_f1,all_val_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDibaBLdbMb3"
      },
      "outputs": [],
      "source": [
        "# #Bar chart showing the training accuracy and validation accuracy w.r.t. different parameter values\n",
        "# labels = (\"Training Accuracy\", \"Validation Accuracy\")\n",
        "# pdata_means = {\n",
        "#     'Count Vectorizer': (all_train_acc[0], all_val_acc[0]),\n",
        "#     'TFIDF': (all_train_acc[1], all_val_acc[1]),\n",
        "#     'GloVe': (all_train_acc[2], all_val_acc[2]),\n",
        "#     'BERT': (all_train_acc[3], all_val_acc[3]),\n",
        "#     'MPNET': (all_train_acc[4], all_val_acc[4]),\n",
        "# }\n",
        "\n",
        "# x = np.arange(len(labels))  # the label locations\n",
        "# width = 0.10  # the width of the bars\n",
        "# multiplier = 0\n",
        "\n",
        "# fig, ax = plt.subplots(layout='constrained')\n",
        "\n",
        "# for attribute, measurement in pdata_means.items():\n",
        "#     print(attribute,measurement)\n",
        "#     offset = width * multiplier\n",
        "#     rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
        "#     ax.bar_label(rects, padding=3)\n",
        "#     multiplier += 1\n",
        "\n",
        "\n",
        "# ax.set_ylabel('Accuracy')\n",
        "# ax.set_title('Accuracy w.r.t Features')\n",
        "# ax.set_xticks(x + width, labels)\n",
        "# ax.legend(loc='best')\n",
        "\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2SqBbnlYkDO"
      },
      "source": [
        "# Other Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ36Yzvn7JUB"
      },
      "source": [
        "### RandomForests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekaaK4lB9Ey1"
      },
      "outputs": [],
      "source": [
        "reset_stat_array()\n",
        "def randomForestsRun(train_val_X, train_val_y):\n",
        "\n",
        "    kf = KFold(n_splits = 5)\n",
        "    train_acc = []\n",
        "    val_acc = []\n",
        "\n",
        "    train_f1 = []\n",
        "    val_f1 = []\n",
        "\n",
        "    train_auc = []\n",
        "    val_auc = []\n",
        "\n",
        "    for train_index, val_index in tqdm(kf.split(train_val_X)):\n",
        "        train_X = train_val_X[train_index[0]:train_index[-1]+1]\n",
        "        train_y = train_val_y[train_index[0]:train_index[-1]+1]\n",
        "\n",
        "        val_X = train_val_X[val_index[0]:val_index[-1]+1]\n",
        "        val_y = train_val_y[val_index[0]:val_index[-1]+1]\n",
        "\n",
        "        dtc = RandomForestClassifier(n_estimators=128, min_samples_leaf=1, max_features=10, random_state=seed)\n",
        "        dtc.fit(train_X, train_y)\n",
        "        train_acc.append(dtc.score(train_X, train_y))\n",
        "        train_f1.append(f1_score(train_y, dtc.predict(train_X), pos_label='positive'))\n",
        "        train_auc.append(roc_auc_score(train_y, dtc.predict_proba(train_X)[:, 1]))\n",
        "\n",
        "        val_acc.append(dtc.score(val_X, val_y))\n",
        "        val_f1.append(f1_score(val_y, dtc.predict(val_X), pos_label='positive'))\n",
        "        val_auc.append(roc_auc_score(val_y, dtc.predict_proba(val_X)[:, 1]))\n",
        "\n",
        "    avg_train_acc = sum(train_acc) / len(train_acc)\n",
        "    avg_val_acc = sum(val_acc) / len(val_acc)\n",
        "\n",
        "    avg_train_f1 = sum(train_f1) / len(train_f1)\n",
        "    avg_val_f1 = sum(val_f1) / len(val_f1)\n",
        "\n",
        "    avg_train_auc = sum(train_auc) / len(train_auc)\n",
        "    avg_val_auc = sum(val_auc) / len(val_auc)\n",
        "    print(\"\\nThe average training accuracy is {} with standard deviation of {}\".format(avg_train_acc,st.pstdev(train_acc)))\n",
        "    print(\"The average validation accuracy is {} with standard deviation of {}\".format(avg_val_acc,st.pstdev(val_acc)))\n",
        "    return avg_train_acc, st.pstdev(train_acc), avg_val_acc, st.pstdev(val_acc), avg_train_f1, avg_val_f1, avg_train_auc, avg_val_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju5qpTKH7HHb",
        "outputId": "85bc41b6-db95-46f3-8424-d95d81be6157"
      },
      "outputs": [],
      "source": [
        "# Evaluate the Random Forests model using 5-fold cross-validation\n",
        "#Count\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = randomForestsRun(X_countvec,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#TFIDF\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = randomForestsRun(X_tfidf,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#Glove\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = randomForestsRun(X_glove,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#BERT\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = randomForestsRun(X_bert,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#MPNET\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = randomForestsRun(X_mpnet,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pubPaLAnYeFE",
        "outputId": "3ed45142-e5b6-476e-e014-93b57be4dda9"
      },
      "outputs": [],
      "source": [
        "drawTable(all_val_acc,all_val_f1,all_val_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9Es_n4qXc9N"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRwuj8hG-LHe"
      },
      "outputs": [],
      "source": [
        "reset_stat_array()\n",
        "def naiveBayesRun(train_val_X, train_val_y):\n",
        "    kf = KFold(n_splits = 5)\n",
        "    train_acc = []\n",
        "    val_acc = []\n",
        "\n",
        "    train_f1 = []\n",
        "    val_f1 = []\n",
        "\n",
        "    train_auc = []\n",
        "    val_auc = []\n",
        "\n",
        "    for train_index, val_index in tqdm(kf.split(train_val_X)):\n",
        "        train_X = train_val_X[train_index[0]:train_index[-1]+1]\n",
        "        train_y = train_val_y[train_index[0]:train_index[-1]+1]\n",
        "\n",
        "        val_X = train_val_X[val_index[0]:val_index[-1]+1]\n",
        "        val_y = train_val_y[val_index[0]:val_index[-1]+1]\n",
        "\n",
        "        gnb = GaussianNB()\n",
        "        gnb.fit(train_X, train_y)\n",
        "\n",
        "        train_acc.append(gnb.score(train_X, train_y))\n",
        "        train_f1.append(f1_score(train_y, gnb.predict(train_X), pos_label='positive'))\n",
        "        train_auc.append(roc_auc_score(train_y, gnb.predict_proba(train_X)[:, 1]))\n",
        "\n",
        "        val_acc.append(gnb.score(val_X, val_y))\n",
        "        val_f1.append(f1_score(val_y, gnb.predict(val_X), pos_label='positive'))\n",
        "        val_auc.append(roc_auc_score(val_y, gnb.predict_proba(val_X)[:, 1]))\n",
        "\n",
        "    avg_train_acc = sum(train_acc) / len(train_acc)\n",
        "    avg_val_acc = sum(val_acc) / len(val_acc)\n",
        "\n",
        "    avg_train_f1 = sum(train_f1) / len(train_f1)\n",
        "    avg_val_f1 = sum(val_f1) / len(val_f1)\n",
        "\n",
        "    avg_train_auc = sum(train_auc) / len(train_auc)\n",
        "    avg_val_auc = sum(val_auc) / len(val_auc)\n",
        "    print(\"\\nThe average training accuracy is {} with standard deviation of {}\".format(avg_train_acc,st.pstdev(train_acc)))\n",
        "    print(\"The average validation accuracy is {} with standard deviation of {}\".format(avg_val_acc,st.pstdev(val_acc)))\n",
        "    return avg_train_acc, st.pstdev(train_acc), avg_val_acc, st.pstdev(val_acc), avg_train_f1, avg_val_f1, avg_train_auc, avg_val_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmFNPrp2CtJm",
        "outputId": "da8f5ec6-4fea-484c-f3fd-489d39e79b15"
      },
      "outputs": [],
      "source": [
        "# Evaluate the Naive Bayes model using 5-fold cross-validation\n",
        "#Count\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = naiveBayesRun(X_countvec,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#TFIDF\n",
        "X_tfidf=X_tensor_tfidf(rt_reviews_data['reviewText'])\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = naiveBayesRun(X_tfidf,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#Glove\n",
        "X_glove=X_tensor_glove(rt_reviews_data['reviewText'])\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = naiveBayesRun(X_glove,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#BERT\n",
        "X_bert=X_tensor_bert(rt_reviews_data['reviewText'])\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = naiveBayesRun(X_bert,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#MPNET\n",
        "X_mpnet=X_tensor_mpnet(rt_reviews_data['reviewText'])\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = naiveBayesRun(X_mpnet,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW3qvMuVC2dY",
        "outputId": "dd768aea-b601-40cb-c5c5-3db73028e9fd"
      },
      "outputs": [],
      "source": [
        "drawTable(all_val_acc,all_val_f1,all_val_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5_9txbHXkJi"
      },
      "source": [
        "### SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JivnaUiBDFWn"
      },
      "outputs": [],
      "source": [
        "reset_stat_array()\n",
        "def sgdRun(train_val_X, train_val_y):\n",
        "\n",
        "    kf = KFold(n_splits = 5)\n",
        "    train_acc = []\n",
        "    val_acc = []\n",
        "\n",
        "    train_f1 = []\n",
        "    val_f1 = []\n",
        "\n",
        "    train_auc = []\n",
        "    val_auc = []\n",
        "\n",
        "    for train_index, val_index in tqdm(kf.split(train_val_X)):\n",
        "        train_X = train_val_X[train_index[0]:train_index[-1]+1]\n",
        "        train_y = train_val_y[train_index[0]:train_index[-1]+1]\n",
        "\n",
        "        val_X = train_val_X[val_index[0]:val_index[-1]+1]\n",
        "        val_y = train_val_y[val_index[0]:val_index[-1]+1]\n",
        "\n",
        "        sgd = make_pipeline(StandardScaler(), SGDClassifier(max_iter=1000, tol=1e-3))\n",
        "        sgd.fit(train_X, train_y)\n",
        "\n",
        "        train_acc.append(sgd.score(train_X, train_y))\n",
        "        train_f1.append(f1_score(train_y, sgd.predict(train_X), pos_label='positive'))\n",
        "        train_auc.append(roc_auc_score(train_y, sgd.decision_function(train_X)))\n",
        "\n",
        "        val_acc.append(sgd.score(val_X, val_y))\n",
        "        val_f1.append(f1_score(val_y, sgd.predict(val_X), pos_label='positive'))\n",
        "        val_auc.append(roc_auc_score(val_y, sgd.decision_function(val_X)))\n",
        "\n",
        "    avg_train_acc = sum(train_acc) / len(train_acc)\n",
        "    avg_val_acc = sum(val_acc) / len(val_acc)\n",
        "\n",
        "    avg_train_f1 = sum(train_f1) / len(train_f1)\n",
        "    avg_val_f1 = sum(val_f1) / len(val_f1)\n",
        "\n",
        "    avg_train_auc = sum(train_auc) / len(train_auc)\n",
        "    avg_val_auc = sum(val_auc) / len(val_auc)\n",
        "    print(\"\\nThe average training accuracy is {} with standard deviation of {}\".format(avg_train_acc,st.pstdev(train_acc)))\n",
        "    print(\"The average validation accuracy is {} with standard deviation of {}\".format(avg_val_acc,st.pstdev(val_acc)))\n",
        "    return avg_train_acc, st.pstdev(train_acc), avg_val_acc, st.pstdev(val_acc), avg_train_f1, avg_val_f1, avg_train_auc, avg_val_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIShy9x3E-FK",
        "outputId": "5860f971-5631-4892-b688-bc86495cdbbe"
      },
      "outputs": [],
      "source": [
        "# Evaluate the SGD model using 5-fold cross-validation\n",
        "#Count\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = sgdRun(X_countvec,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#TFIDF\n",
        "X_tfidf=X_tensor_tfidf(rt_reviews_data['reviewText'])\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = sgdRun(X_tfidf,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#Glove\n",
        "X_glove=X_tensor_glove(rt_reviews_data['reviewText'])\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = sgdRun(X_glove,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#BERT\n",
        "X_bert=X_tensor_bert(rt_reviews_data['reviewText'])\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = sgdRun(X_bert,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)\n",
        "\n",
        "#MPNET\n",
        "X_mpnet=X_tensor_mpnet(rt_reviews_data['reviewText'])\n",
        "train_acc, train_std, val_acc, val_std, train_f1, val_f1, train_auc, val_auc = sgdRun(X_mpnet,Y)\n",
        "\n",
        "all_train_acc.append(train_acc)\n",
        "all_train_std.append(train_std)\n",
        "all_train_f1.append(train_f1)\n",
        "all_train_auc.append(train_auc)\n",
        "\n",
        "all_val_acc.append(val_acc)\n",
        "all_val_std.append(val_std)\n",
        "all_val_f1.append(val_f1)\n",
        "all_val_auc.append(val_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBKedlXpYQRd",
        "outputId": "a6e524dd-cd63-4ec0-d56f-9d9f8e0d394d"
      },
      "outputs": [],
      "source": [
        "drawTable(all_val_acc,all_val_f1,all_val_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMw-IgLTOzvE"
      },
      "source": [
        "# Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4s3kClKPu3rO",
        "outputId": "ad7c826f-989b-4286-f3ae-8692b6ecf23f"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-distilroberta-v1\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"sentence-transformers/all-distilroberta-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PspR-x8IQ98V"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets\n",
        "import datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(rt_reviews_data['reviewText'],rt_reviews_data['scoreSentiment'],test_size=0.2, shuffle=True, random_state=seed)\n",
        "\n",
        "dataset_train={'text':X_train, 'labels':[1 if x=='positive' else 0 for x in list(y_train)]}\n",
        "dataset_test={'text':X_test, 'labels':[1 if x=='positive' else 0 for x in list(y_test)]}\n",
        "\n",
        "dataset_train=datasets.Dataset.from_pandas(pd.DataFrame(data=dataset_train))\n",
        "dataset_test=datasets.Dataset.from_pandas(pd.DataFrame(data=dataset_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "44eb252a5e304d2fa73f5f1e7e17eac7",
            "7e09272740984f3d81fe5f0a9fbcfba2"
          ]
        },
        "id": "kBczt5HXPWDs",
        "outputId": "f63596b0-0d62-4b4b-e312-1da2d12fabfa"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "   return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_train = dataset_train.map(preprocess_function, batched=True)\n",
        "tokenized_test = dataset_test.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5prh9IRDPi8F"
      },
      "outputs": [],
      "source": [
        "# tokenized_train = tokenized_train.remove_columns('text')\n",
        "# tokenized_test = tokenized_test.remove_columns('text')\n",
        "\n",
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mcG1C9IPlMd"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoModelForSequenceClassification\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\", num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJqR1FigPolM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "   load_accuracy = load_metric(\"accuracy\")\n",
        "   load_f1 = load_metric(\"f1\")\n",
        "\n",
        "   logits, labels = eval_pred\n",
        "   predictions = np.argmax(logits, axis=-1)\n",
        "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "   return {\"accuracy\": accuracy, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOWZkMxLRxxH",
        "outputId": "7c84c107-78d9-4b1c-c7b0-32f1e20ce891"
      },
      "outputs": [],
      "source": [
        "# RUN this first, Comment it and then restart session and run all.\n",
        "# !pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnEu2YoYU0j5",
        "outputId": "cabf4431-634a-4f3e-d2ff-922004116c3d"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8QrP-_LQVhf",
        "outputId": "bf44f6ab-54f4-4fe4-f167-541bf6ed6c99"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "repo_name = \"finetuning-sentiment-model-distilbert-imdb\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "   output_dir=repo_name,\n",
        "   learning_rate=2e-5,\n",
        "   per_device_train_batch_size=16,\n",
        "   per_device_eval_batch_size=16,\n",
        "   num_train_epochs=2,\n",
        "   weight_decay=0.01,\n",
        "   save_strategy=\"epoch\",\n",
        "   push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_test,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "Oq9LvWhAkkuZ",
        "outputId": "d88f7d7f-1387-41a2-c978-a10c3bcfb662"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416,
          "referenced_widgets": [
            "d19c10ec66a143b68b98576fc33ddfd9",
            "e9284d21715540739b689c6d9320227a"
          ]
        },
        "id": "galNB1J-ki_G",
        "outputId": "fd7f5746-544a-4dfd-8d41-4b074ebb6b89"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "bd7fea66b3894ad6a93b5b972174caff"
          ]
        },
        "id": "7f7UHRCQkm-f",
        "outputId": "2d526972-e86c-42f0-b693-3952d989625f"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
